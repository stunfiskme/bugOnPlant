{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bab12da",
   "metadata": {},
   "source": [
    "A neural network is a computational model inspired by the way biological neurons work in the human brain. It consists of interconnected nodes or neurons that process and learn from data, enabling tasks such as pattern recognition and decision-making in machine learning.\n",
    "\n",
    "Things to know:\n",
    "\n",
    "Activation function(see note): A function is applied to the output of each neuron to introduce non-linearity into the network. Common examples include ReLU (Rectified Linear Unit) and sigmoid functions.\n",
    "\n",
    "Hyperparameter:A parameter that is set before the training process begins, such as learning rate or number of layers. Unlike model parameters, Hyperparameters are not learned from the training data.\n",
    "\n",
    "Training loss: The loss computed on the training data during the model training process. Monitoring this helps assess how well the model is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2ece4",
   "metadata": {},
   "source": [
    "This code implements a complete training and evaluation pipeline for a neural network on the FashionMNIST dataset using PyTorch. It first sets key hyperparameters such as the learning rate, batch size, and number of epochs. The FashionMNIST dataset is downloaded and loaded, with images transformed into tensors suitable for the model. DataLoaders are created to batch the data for training and testing. The device is set to GPU if available, otherwise CPU. The neural network model is defined with a flatten layer that converts each 28x28 image into a 784-length vector, followed by a sequence of two fully connected layers with 512 neurons each and ReLU activations, and a final output layer producing 10 logits for the classes. The `train_loop` function handles one epoch of training by iterating over batches, computing predictions, calculating the loss with cross-entropy, performing backpropagation, and updating weights using stochastic gradient descent. The `test_loop` function evaluates the modelâ€™s performance on the test set without computing gradients, calculating average loss and accuracy. Finally, the code runs the training and testing loops for the specified number of epochs, printing progress and results at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300700  [   64/60000]\n",
      "loss: 2.287054  [ 6464/60000]\n",
      "loss: 2.267402  [12864/60000]\n",
      "loss: 2.265351  [19264/60000]\n",
      "loss: 2.245767  [25664/60000]\n",
      "loss: 2.202098  [32064/60000]\n",
      "loss: 2.229139  [38464/60000]\n",
      "loss: 2.186772  [44864/60000]\n",
      "loss: 2.184453  [51264/60000]\n",
      "loss: 2.156507  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.145272 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.157872  [   64/60000]\n",
      "loss: 2.146770  [ 6464/60000]\n",
      "loss: 2.083392  [12864/60000]\n",
      "loss: 2.108032  [19264/60000]\n",
      "loss: 2.054494  [25664/60000]\n",
      "loss: 1.976589  [32064/60000]\n",
      "loss: 2.027245  [38464/60000]\n",
      "loss: 1.937592  [44864/60000]\n",
      "loss: 1.938737  [51264/60000]\n",
      "loss: 1.876564  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.862501 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.895978  [   64/60000]\n",
      "loss: 1.865718  [ 6464/60000]\n",
      "loss: 1.736500  [12864/60000]\n",
      "loss: 1.792272  [19264/60000]\n",
      "loss: 1.682754  [25664/60000]\n",
      "loss: 1.619408  [32064/60000]\n",
      "loss: 1.661692  [38464/60000]\n",
      "loss: 1.558082  [44864/60000]\n",
      "loss: 1.571302  [51264/60000]\n",
      "loss: 1.482511  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.488459 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.554532  [   64/60000]\n",
      "loss: 1.522470  [ 6464/60000]\n",
      "loss: 1.362294  [12864/60000]\n",
      "loss: 1.451268  [19264/60000]\n",
      "loss: 1.333835  [25664/60000]\n",
      "loss: 1.315356  [32064/60000]\n",
      "loss: 1.348181  [38464/60000]\n",
      "loss: 1.271361  [44864/60000]\n",
      "loss: 1.293889  [51264/60000]\n",
      "loss: 1.211584  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.228283 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.303990  [   64/60000]\n",
      "loss: 1.287351  [ 6464/60000]\n",
      "loss: 1.114981  [12864/60000]\n",
      "loss: 1.235825  [19264/60000]\n",
      "loss: 1.112668  [25664/60000]\n",
      "loss: 1.121475  [32064/60000]\n",
      "loss: 1.162010  [38464/60000]\n",
      "loss: 1.096826  [44864/60000]\n",
      "loss: 1.124521  [51264/60000]\n",
      "loss: 1.056239  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 1.069901 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.140870  [   64/60000]\n",
      "loss: 1.142584  [ 6464/60000]\n",
      "loss: 0.954469  [12864/60000]\n",
      "loss: 1.102239  [19264/60000]\n",
      "loss: 0.977371  [25664/60000]\n",
      "loss: 0.994090  [32064/60000]\n",
      "loss: 1.049904  [38464/60000]\n",
      "loss: 0.986994  [44864/60000]\n",
      "loss: 1.014333  [51264/60000]\n",
      "loss: 0.959908  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.968392 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.028029  [   64/60000]\n",
      "loss: 1.049535  [ 6464/60000]\n",
      "loss: 0.844897  [12864/60000]\n",
      "loss: 1.012610  [19264/60000]\n",
      "loss: 0.892228  [25664/60000]\n",
      "loss: 0.905506  [32064/60000]\n",
      "loss: 0.977790  [38464/60000]\n",
      "loss: 0.915455  [44864/60000]\n",
      "loss: 0.938292  [51264/60000]\n",
      "loss: 0.894907  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.899176 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.944795  [   64/60000]\n",
      "loss: 0.984822  [ 6464/60000]\n",
      "loss: 0.766651  [12864/60000]\n",
      "loss: 0.948133  [19264/60000]\n",
      "loss: 0.835349  [25664/60000]\n",
      "loss: 0.840889  [32064/60000]\n",
      "loss: 0.927322  [38464/60000]\n",
      "loss: 0.867124  [44864/60000]\n",
      "loss: 0.883294  [51264/60000]\n",
      "loss: 0.847577  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.848960 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.880122  [   64/60000]\n",
      "loss: 0.935844  [ 6464/60000]\n",
      "loss: 0.707914  [12864/60000]\n",
      "loss: 0.899600  [19264/60000]\n",
      "loss: 0.794463  [25664/60000]\n",
      "loss: 0.792122  [32064/60000]\n",
      "loss: 0.888673  [38464/60000]\n",
      "loss: 0.832680  [44864/60000]\n",
      "loss: 0.841719  [51264/60000]\n",
      "loss: 0.810847  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.810492 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.827838  [   64/60000]\n",
      "loss: 0.895884  [ 6464/60000]\n",
      "loss: 0.662287  [12864/60000]\n",
      "loss: 0.861692  [19264/60000]\n",
      "loss: 0.763007  [25664/60000]\n",
      "loss: 0.754322  [32064/60000]\n",
      "loss: 0.856853  [38464/60000]\n",
      "loss: 0.806715  [44864/60000]\n",
      "loss: 0.809083  [51264/60000]\n",
      "loss: 0.780859  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.779522 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#hyperparameters \n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "#loss function \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
